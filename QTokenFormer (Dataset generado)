!pip install pennylane
import torch
import torch.nn as nn
from torch.nn import functional as F
import pennylane as qml
from pennylane import numpy as np
import copy
import math
import time
import os
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Crear carpetas necesarias
os.makedirs("graficas_finales", exist_ok=True)
os.makedirs("checkpoints_hubo", exist_ok=True)

# ==========================================
# 1. PREPARACIÃ“N DE DATOS Y HELPERS
# ==========================================

def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=2048):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)
    def forward(self, x): return x + self.pe[:, :x.size(1), :]

def get_complex_batch(batch_size, seq_len, vocab_size):
    # Datos sintÃ©ticos de complejidad media-alta (Patrones + Ruido)
    cycle = torch.arange(vocab_size)
    full_pattern = cycle.repeat(max(2, (seq_len // vocab_size) + 2))
    input_data = torch.zeros((batch_size, seq_len), dtype=torch.long)
    target_data = torch.zeros((batch_size, seq_len), dtype=torch.long)

    for i in range(batch_size):
        limit = len(full_pattern) - seq_len - 2
        start = torch.randint(0, limit, (1,)).item() if limit > 0 else 0
        seq = full_pattern[start : start + seq_len + 1].clone()
        if torch.rand(1) < 0.2: # 20% de probabilidad de ruido
            noise_idx = torch.randint(0, seq_len, (int(seq_len*0.15) + 1,))
            seq[noise_idx] = torch.randint(0, vocab_size, noise_idx.shape)
        input_data[i] = seq[:-1]; target_data[i] = seq[1:]
    return input_data, target_data

def create_fixed_dataset(n_samples, seq_len, vocab_size):
    set_seed(42)
    inputs_list, targets_list = [], []
    for _ in range(max(1, n_samples // 10)):
        i, t = get_complex_batch(10, seq_len, vocab_size)
        inputs_list.append(i); targets_list.append(t)
    return torch.cat(inputs_list), torch.cat(targets_list)

def train_model(model, model_name, n_epochs, steps_per_epoch, val_inputs, val_targets, batch_size, vocab_size, seq_len, lr):
    print(f"\nðŸ”¹ {model_name} | Params: {count_parameters(model):,}")
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=steps_per_epoch, epochs=n_epochs, pct_start=0.1)

    val_losses = []
    val_size = val_inputs.shape[0]

    for epoch in range(n_epochs):
        model.train()
        for _ in range(steps_per_epoch):
            inputs, targets = get_complex_batch(batch_size, seq_len, vocab_size)
            optimizer.zero_grad()
            logits = model(inputs)
            loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

        # ValidaciÃ³n
        model.eval(); val_loss_acc = 0; num_batches = 0
        with torch.no_grad():
            for i in range(0, val_size, batch_size):
                inp = val_inputs[i : i + batch_size]
                tgt = val_targets[i : i + batch_size]
                if inp.shape[0] == 0: continue
                val_loss_acc += criterion(model(inp).reshape(-1, vocab_size), tgt.reshape(-1)).item()
                num_batches += 1

        avg_val = val_loss_acc / max(num_batches, 1)
        val_losses.append(avg_val)

        # Guardar Checkpoint si es HUBO (Para luego usar en IBM si se quiere)
        if "Q-HUBO" in model_name:
            torch.save(model.state_dict(), f"checkpoints_hubo/{model_name}_ep{epoch+1}.pt")

        if (epoch + 1) % 2 == 0 or epoch == 0:
            print(f"   Ep {epoch+1}: Val Loss {avg_val:.4f}")

    parts = model_name.split("-")
    return {
        "name": model_name,
        "type": f"{parts[0]}-{parts[1]}",
        "scale": parts[2],
        "val_curve": val_losses,
        "loss": val_losses[-1],
        "params": count_parameters(model)
    }

# ==========================================
# 2. LÃ“GICA CUÃNTICA & HUBO v4
# ==========================================
N_QUBITS = 4
N_LAYERS = 2
dev = qml.device("default.qubit", wires=N_QUBITS)

@qml.qnode(dev, interface="torch")
def circuit_generic(inputs, weights):
    qml.AngleEmbedding(inputs, wires=range(N_QUBITS))
    qml.StronglyEntanglingLayers(weights, wires=range(N_QUBITS))
    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]

@qml.qnode(dev, interface="torch")
def circuit_hubo(inputs, weights):
    # HUBO: Re-uploading + Rotaciones Y (Reales)
    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation='Y')
    qml.BasicEntanglerLayers(weights[0:1], wires=range(N_QUBITS), rotation=qml.RY)

    qml.AngleEmbedding(inputs, wires=range(N_QUBITS), rotation='Y')
    qml.BasicEntanglerLayers(weights[1:2], wires=range(N_QUBITS), rotation=qml.RY)
    return [qml.expval(qml.PauliZ(i)) for i in range(N_QUBITS)]

class QuantumLinear(nn.Module):
    """Capa CuÃ¡ntica GenÃ©rica:"""
    def __init__(self, in_feat, out_feat, n_qubits=N_QUBITS, n_layers=N_LAYERS):
        super().__init__()
        self.pre_proj = nn.Linear(in_feat, n_qubits)
        self.q_layer = qml.qnn.TorchLayer(circuit_generic, {"weights": (n_layers, n_qubits, 3)})
        self.post_proj = nn.Linear(n_qubits, out_feat)
        # Init TÃ³xica (Aleatoria completa)
        with torch.no_grad():
            self.q_layer.weights.data.uniform_(0, 2 * np.pi)

    def forward(self, x):
        x = self.pre_proj(x)
        x = torch.tanh(x) * (np.pi / 2)
        b_shape = x.shape
        q_out = self.q_layer(x.reshape(-1, b_shape[-1])).reshape(b_shape)
        return self.post_proj(q_out)

class HUBOQuantumLinear(nn.Module):
    """HUBO v4: InicializaciÃ³n de Identidad Estricta + Residual."""
    def __init__(self, in_feat, out_feat, n_qubits=N_QUBITS, n_layers=N_LAYERS):
        super().__init__()
        self.classical_path = nn.Linear(in_feat, out_feat)
        self.q_norm = nn.LayerNorm(in_feat)
        self.pre_proj = nn.Linear(in_feat, n_qubits)
        self.q_layer = qml.qnn.TorchLayer(circuit_hubo, {"weights": (n_layers, n_qubits)})
        self.post_proj = nn.Linear(n_qubits, out_feat)
        self.alpha = nn.Parameter(torch.tensor(0.1))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.classical_path.weight)
        nn.init.zeros_(self.classical_path.bias)
        nn.init.orthogonal_(self.pre_proj.weight)
        nn.init.xavier_uniform_(self.post_proj.weight, gain=0.01)
        nn.init.zeros_(self.post_proj.bias)
        # Identidad: Ceros exactos en el circuito
        with torch.no_grad():
            self.q_layer.weights.data.fill_(0.0)

    def forward(self, x):
        out_classical = self.classical_path(x)
        q_in = torch.atan(self.pre_proj(self.q_norm(x))) * 2.0
        b_shape = q_in.shape
        q_out = self.q_layer(q_in.reshape(-1, b_shape[-1])).reshape(b_shape)
        return out_classical + (self.alpha * self.post_proj(q_out))

# ==========================================
# 3. ARQUITECTURA UNIVERSAL
# ==========================================
class GEGLU(nn.Module):
    def forward(self, x): x, g = x.chunk(2, dim=-1); return x * F.gelu(g)

class TokenFormerFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d_model, d_ff*2), GEGLU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model))
    def forward(self, x): return self.net(x)

class VanillaFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model))
    def forward(self, x): return self.net(x)

# Custom Attention Wrapper para capas cuÃ¡nticas
class CustomAttention(nn.Module):
    def __init__(self, d_model, nhead, LinearCls):
        super().__init__()
        self.nhead = nhead; self.dk = d_model // nhead
        self.q = LinearCls(d_model, d_model)
        self.k = LinearCls(d_model, d_model)
        self.v = LinearCls(d_model, d_model)
        self.o = LinearCls(d_model, d_model)

    def forward(self, x, mask=None):
        B, S, D = x.shape
        Q = self.q(x).view(B, S, self.nhead, self.dk).transpose(1, 2)
        K = self.k(x).view(B, S, self.nhead, self.dk).transpose(1, 2)
        V = self.v(x).view(B, S, self.nhead, self.dk).transpose(1, 2)
        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.dk)
        if mask is not None: scores = scores.masked_fill(mask == 0, float('-inf'))
        attn = F.softmax(scores, dim=-1)
        return self.o((attn @ V).transpose(1, 2).contiguous().view(B, S, D))

class UniversalDecoderLayer(nn.Module):
    def __init__(self, d_model, nhead, d_ff, dropout, mode):
        super().__init__()
        if "Vanilla" in mode: Linear, FFN = nn.Linear, VanillaFeedForward
        elif "TokenFormer" in mode: Linear, FFN = nn.Linear, TokenFormerFeedForward
        elif "Q-Generic" in mode: Linear, FFN = QuantumLinear, None
        elif "Q-HUBO" in mode: Linear, FFN = HUBOQuantumLinear, None

        # ðŸ› ï¸ SELECCIÃ“N DE ATENCIÃ“N (Fix del Error)
        if "Q-" in mode:
             self.attn = CustomAttention(d_model, nhead, Linear)
        else:
             # PyTorch nativo para clÃ¡sicos (optimizado)
             self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)

        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        if FFN: self.ffn = FFN(d_model, d_ff, dropout)
        else: self.ffn = nn.Sequential(Linear(d_model, d_ff), nn.ReLU(), Linear(d_ff, d_model))

    def forward(self, x, mask):
        # ðŸ› ï¸ LOGICA DE FORWARD CORREGIDA
        x_norm = self.norm1(x)
        if isinstance(self.attn, nn.MultiheadAttention):
            attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask)
        else:
            attn_out = self.attn(x_norm, mask=mask)

        x = x + self.dropout(attn_out)
        x = x + self.dropout(self.ffn(self.norm2(x)))
        return x

class UniversalTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, d_ff, max_len, mode):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.pos = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([UniversalDecoderLayer(d_model, nhead, d_ff, 0.1, mode) for _ in range(num_layers)])
        if "Q-HUBO" in mode: self.head = HUBOQuantumLinear(d_model, vocab_size)
        elif "Q-Generic" in mode: self.head = QuantumLinear(d_model, vocab_size)
        else: self.head = nn.Linear(d_model, vocab_size)
        self.d_model = d_model

    def forward(self, x):
        mask = torch.triu(torch.ones(x.size(1), x.size(1)), diagonal=1).to(x.device)
        mask = (mask == 0).float()
        x = self.pos(self.emb(x) * math.sqrt(self.d_model))
        for l in self.layers: x = l(x, mask)
        return self.head(x)

# ==========================================
# 4. EJECUCIÃ“N (8 MODELOS)
# ==========================================
VOCAB_SIZE = 128; SEQ_LEN = 32; BATCH_SIZE = 16; N_EPOCHS = 10; STEPS = 120; LR = 3e-3
print("--- ðŸ INICIANDO SIMULACIÃ“N COMPLETA (HUBO v4) ---")

val_in, val_tgt = create_fixed_dataset(64, SEQ_LEN, VOCAB_SIZE)
results = []

configs = [
    ("C-Vanilla-Medium", {"d_model": 32, "nhead": 4, "num_layers": 2, "d_ff": 64}),
    ("C-TokenFormer-Tiny", {"d_model": 32, "nhead": 4, "num_layers": 1, "d_ff": 64}),
    ("C-TokenFormer-Medium", {"d_model": 32, "nhead": 4, "num_layers": 2, "d_ff": 64}),
    ("Q-Generic-Tiny", {"d_model": 32, "nhead": 4, "num_layers": 1, "d_ff": 32}),
    ("Q-HUBO-Tiny", {"d_model": 32, "nhead": 4, "num_layers": 1, "d_ff": 32}),
    ("Q-Generic-Medium", {"d_model": 32, "nhead": 4, "num_layers": 2, "d_ff": 32}),
    ("Q-HUBO-Medium", {"d_model": 32, "nhead": 4, "num_layers": 2, "d_ff": 32}),
]

for name, cfg in configs:
    set_seed(42)
    mode = "-".join(name.split("-")[:2])
    model = UniversalTransformer(VOCAB_SIZE, cfg["d_model"], cfg["nhead"], cfg["num_layers"], cfg["d_ff"], SEQ_LEN, mode)
    this_lr = 5e-3 if "Q-Generic" in name else LR
    results.append(train_model(model, name, N_EPOCHS, STEPS, val_in, val_tgt, BATCH_SIZE, VOCAB_SIZE, SEQ_LEN, this_lr))

# ==========================================
# 5. GENERACIÃ“N DE GRÃFICAS
# ==========================================
df = pd.DataFrame(results)
save_dir = Path("graficas_finales")

def plot_compare(filter_names, title, filename):
    plt.figure(figsize=(10, 6))
    subset = df[df["name"].isin(filter_names)]
    colors = {"C-Vanilla": "blue", "C-TokenFormer": "green", "Q-Generic": "orange", "Q-HUBO": "red"}
    styles = {"Tiny": "--", "Medium": "-"}

    for _, row in subset.iterrows():
        lbl = row['name']
        c_key = "-".join(lbl.split("-")[:2])
        s_key = lbl.split("-")[-1]
        plt.plot(row['val_curve'], label=lbl, color=colors[c_key], linestyle=styles[s_key], linewidth=2.5)

    plt.title(title, fontsize=14); plt.xlabel("Ã‰poca"); plt.ylabel("Loss"); plt.legend(); plt.grid(True, alpha=0.3)
    plt.savefig(save_dir / filename)
    plt.show()

print("\n--- ðŸ“Š GENERANDO GRÃFICAS ---")
plot_compare(["C-Vanilla-Medium", "C-TokenFormer-Tiny", "C-TokenFormer-Medium"],
             "1. Mejora ClÃ¡sica: Vanilla vs TokenFormer", "1_classic_scaling.png")

plot_compare(["C-TokenFormer-Tiny", "C-TokenFormer-Medium", "Q-Generic-Tiny", "Q-Generic-Medium"],
             "2. El Reto CuÃ¡ntico: ClÃ¡sico vs Generic", "2_classic_vs_quantum.png")

plot_compare(["Q-Generic-Tiny", "Q-Generic-Medium", "Q-HUBO-Tiny", "Q-HUBO-Medium"],
             "3. SoluciÃ³n HUBO: Eliminando Valles EstÃ©riles", "3_hubo_effect.png")

print(df[["name", "loss", "params"]].sort_values("loss").to_markdown(index=False))
